{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright 2019 The Google Research Authors.\n",
    "\n",
    "Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "you may not use this file except in compliance with the License.\n",
    "You may obtain a copy of the License at\n",
    "\n",
    "     http://www.apache.org/licenses/LICENSE-2.0\n",
    "\n",
    "Unless required by applicable law or agreed to in writing, software\n",
    "distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "See the License for the specific language governing permissions and\n",
    "limitations under the License."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Corrupted Sample Discovery & Robust Learning using DVRL\n",
    "\n",
    " * Jinsung Yoon, Sercan O Arik, Tomas Pfister, \"Data Valuation using Reinforcement Learning\", arXiv preprint arXiv:1909.11671 (2019) - https://arxiv.org/abs/1909.11671\n",
    "\n",
    "This notebook describes the user-guide of corrupted sample discovery and robust learning applications using \"Data Valuation using Reinforcement Learning (DVRL)\". \n",
    "\n",
    "There are some scenarios where training samples may contain corrupted samples, e.g. due to cheap label collection methods. An automated corrupted sample discovery method would be highly beneficial for distinguishing samples with clean vs. noisy labels. Data valuation can be used in this setting by having a small clean validation set to assign low data values to the potential samples with noisy labels. With an optimal data value estimator, all noisy labels would get the lowest data values. \n",
    "\n",
    "DVRL can also reliably learn with noisy data in an end-to-end way. Ideally, noisy samples should get low data values as DVRL converges and a high performance model can be returned.\n",
    "\n",
    "You need:\n",
    "\n",
    "**Training set** (low-quality data (e.g. noisy data)) / **Validation set** (high-quality data (e.g. clean data)) / **Testing set** (high-quality data (e.g. clean data)) \n",
    " * If there is no explicit validation set, you can split a small portion of testing set as the validation set. \n",
    " * Note that training set does not have to be low quality for DVRL; however, in this notebook, we use a low quality training set for a more clear demonstration as the samples are easier to distinguish in terms of their value.\n",
    " * If you have your own training / validation / testing datasets, you can put them under './repo/data_files/' directory with 'train.csv', 'valid.csv', 'test.csv' names.\n",
    " * In this notebook, we use adult income dataset (https://archive.ics.uci.edu/ml/datasets/Adult) as an example."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Prerequisite\n",
    "\n",
    " * Download lightgbm package.\n",
    " * Clone https://github.com/google-research/google-research.git to the current directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: lightgbm in d:\\anaconda\\lib\\site-packages (3.1.1)\n",
      "Requirement already satisfied: scikit-learn!=0.22.0 in d:\\anaconda\\lib\\site-packages (from lightgbm) (0.23.1)\n",
      "Requirement already satisfied: scipy in d:\\anaconda\\lib\\site-packages (from lightgbm) (1.5.0)\n",
      "Requirement already satisfied: numpy in d:\\anaconda\\lib\\site-packages (from lightgbm) (1.19.4)\n",
      "Requirement already satisfied: wheel in d:\\anaconda\\lib\\site-packages (from lightgbm) (0.36.2)\n",
      "Requirement already satisfied: joblib>=0.11 in d:\\anaconda\\lib\\site-packages (from scikit-learn!=0.22.0->lightgbm) (0.16.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in d:\\anaconda\\lib\\site-packages (from scikit-learn!=0.22.0->lightgbm) (2.1.0)\n",
      "yes\n"
     ]
    }
   ],
   "source": [
    "# Uses pip3 to install necessary package (lightgbm)\n",
    "!pip3 install lightgbm\n",
    "\n",
    "# Resets the IPython kernel to import the installed package.\n",
    "import IPython\n",
    "app = IPython.Application.instance()\n",
    "app.kernel.do_shutdown(True)  \n",
    "print('yes')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "yes\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from git import Repo\n",
    "\n",
    "# Current working directory\n",
    "repo_dir = os.getcwd() + '/repo'\n",
    "\n",
    "if not os.path.exists(repo_dir):\n",
    "    os.makedirs(repo_dir)\n",
    "\n",
    "# Clones github repository\n",
    "if not os.listdir(repo_dir):\n",
    "    git_url = \"https://github.com/google-research/google-research.git\"\n",
    "    Repo.clone_from(git_url, repo_dir)\n",
    "print('yes')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Necessary packages and functions call\n",
    "\n",
    " * load_tabular_data: Data loader for tabular datasets.\n",
    " * data_preprocess: Data extraction and normalization.\n",
    " * dvrl_classification: Data valuation function for classification problem.\n",
    " * metrics: Evaluation metrics of the quality of data valuation in various metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From D:\\Anaconda\\lib\\site-packages\\tensorflow\\python\\compat\\v2_compat.py:96: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "non-resource variables are not supported in the long term\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'os' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-abe084b20bd3>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;31m# Sets current directory\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mchdir\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrepo_dir\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mdata_loading\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mload_tabular_data\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpreprocess_data\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'os' is not defined"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow.compat.v1 as tf\n",
    "tf.disable_v2_behavior()\n",
    "from sklearn import linear_model\n",
    "import lightgbm\n",
    "\n",
    "# Sets current directory\n",
    "os.chdir(repo_dir)\n",
    "\n",
    "from data_loading import load_tabular_data, preprocess_data\n",
    "import dvrl\n",
    "from dvrl_metrics import discover_corrupted_sample, remove_high_low, learn_with_dvrl\n",
    "print('yes')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data loading & Sample corruption\n",
    "\n",
    " * Create training dataset, validation and testing datasets, and save as train.csv, valid.csv, test.csv under './repo/data_files/' directory.\n",
    " * In this notebook, we corrupt a certain portion of samples in training set to create \"artificially\" low-quality data.\n",
    " * If you have your own train.csv (low-quality data), valid.csv (ideally high-quality data), test.csv (ideally similar to validation distribution), you can skip this cell and just save those files to './repo/data_files/' directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "yeah\n",
      "oh\n",
      "ok\n",
      "Finished data loading.\n"
     ]
    }
   ],
   "source": [
    "# Data name: 'adult' in this notebook\n",
    "data_name = 'adult'\n",
    "\n",
    "# The number of training and validation samples\n",
    "dict_no = dict()\n",
    "dict_no['train'] = 1000\n",
    "dict_no['valid'] = 400\n",
    "\n",
    "# Label noise ratio\n",
    "noise_rate = 0.2\n",
    "\n",
    "# Loads data and corrupts labels\n",
    "noise_idx = load_tabular_data(data_name, dict_no, noise_rate)\n",
    "# noise_idx: ground truth noisy sample indices\n",
    "\n",
    "print('Finished data loading.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preprocessing\n",
    "\n",
    " * Extract features and labels from train.csv, valid.csv, test.csv in './repo/data_files/' directory.\n",
    " * Normalize the features of training, validation, and testing sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished data preprocess.\n"
     ]
    }
   ],
   "source": [
    "# Normalization methods: 'minmax' or 'standard'\n",
    "normalization = 'minmax' \n",
    "\n",
    "# Extracts features and labels. Then, normalizes features.\n",
    "x_train, y_train, x_valid, y_valid, x_test, y_test, _ = \\\n",
    "preprocess_data(normalization, 'train.csv', 'valid.csv', 'test.csv')\n",
    "\n",
    "print('Finished data preprocess.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run DVRL\n",
    "\n",
    "1. **Input**: \n",
    "\n",
    " * data valuator network parameters: Set network parameters of data valuator.\n",
    " * pred_model: The predictor model that maps output from the input. Any machine learning model (e.g. a neural network or ensemble decision tree) can be used as the predictor model, as long as it has fit, and predict (for regression)/predict_proba (for classification) as its subfunctions. Fit can be implemented using multiple backpropagation iterations.\n",
    " \n",
    " \n",
    "2. **Output**:\n",
    " * data_valuator: Function that uses training set as inputs to estimate data values.\n",
    " * dvrl_predictor: Function that predicts labels of the testing samples.\n",
    " * dve_out: Estimated data values for all training samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'tensorflow' has no attribute 'reset_default_graph'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-6-9c050f4a277f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# Resets the graph\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreset_default_graph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;31m# Network parameters\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mparameters\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: module 'tensorflow' has no attribute 'reset_default_graph'"
     ]
    }
   ],
   "source": [
    "# Resets the graph\n",
    "tf.reset_default_graph()\n",
    "\n",
    "# Network parameters\n",
    "parameters = dict()\n",
    "parameters['hidden_dim'] = 100\n",
    "parameters['comb_dim'] = 10\n",
    "parameters['iterations'] = 2000\n",
    "parameters['activation'] = tf.nn.relu\n",
    "parameters['layer_number'] = 5\n",
    "parameters['batch_size'] = 2000\n",
    "parameters['learning_rate'] = 0.01\n",
    "\n",
    "# Sets checkpoint file name\n",
    "checkpoint_file_name = './tmp/model.ckpt'\n",
    "\n",
    "# Defines predictive model\n",
    "pred_model = linear_model.LogisticRegression(solver='lbfgs')\n",
    "problem = 'classification'\n",
    "\n",
    "# Flags for using stochastic gradient descent / pre-trained model\n",
    "flags = {'sgd': False, 'pretrain': False}\n",
    "\n",
    "# Initalizes DVRL\n",
    "dvrl_class = dvrl.Dvrl(x_train, y_train, x_valid, y_valid, \n",
    "                       problem, pred_model, parameters, checkpoint_file_name, flags)\n",
    "\n",
    "# Trains DVRL\n",
    "dvrl_class.train_dvrl('auc')\n",
    "\n",
    "print('Finished dvrl training.')\n",
    "\n",
    "# Estimates data values\n",
    "dve_out = dvrl_class.data_valuator(x_train, y_train)\n",
    "\n",
    "# Predicts with DVRL\n",
    "y_test_hat = dvrl_class.dvrl_predictor(x_test)\n",
    "\n",
    "print('Finished data valuation.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluations\n",
    "\n",
    " * In this notebook, we use LightGBM as the predictive model in DVRL (but we can also replace it with another method for evaluation purposes.\n",
    " * Here, we use average accuracy as the performance metric (we can also replace with other metrics like AUC, see metrics.py)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Robust learning\n",
    "\n",
    "DVRL learns robustly although the training data contains low quality/noisy samples, using the guidance from the high quality/clean validation data via reinforcement learning.\n",
    " * Train predictive model with weighted optimization using estimated data values by DVRL as the weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defines evaluation model\n",
    "eval_model = lightgbm.LGBMClassifier()\n",
    "\n",
    "# Robust learning (DVRL-weighted learning)\n",
    "robust_perf = learn_with_dvrl(dve_out, eval_model, \n",
    "                              x_train, y_train, x_valid, y_valid, x_test, y_test, 'accuracy')\n",
    "\n",
    "print('DVRL-weighted learning performance: ' + str(np.round(robust_perf, 4)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Removing high/low valued samples\n",
    "\n",
    "Removing low value samples from the training dataset can improve the predictor model performance, especially in the cases where the training dataset contains corrupted samples. On the other\n",
    "hand, removing high value samples, especially if the dataset is small, would decrease the performance significantly. Overall, the performance after removing high/low value samples is a strong\n",
    "indicator for the quality of data valuation.\n",
    "\n",
    "DVRL can rank the training data samples according to their estimated data value, and **by removing the low value samples we can significantly improve performance, whereas removing the high value samples degrades the performance severely. Thus for a high performance data valuation method, a large gap is expected in the performance curves with removal of high vs. low value samples**\n",
    " * Train predictive models after removing certain portions of high/low valued training samples.\n",
    " * Visualize the results using line graphs (set plot = True).\n",
    " * x-axis: Portions of removed samples.\n",
    " * y-axis: Prediction performance (accuracy).\n",
    " * Blue line: Removing low value data, Orange line: Removing high value data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluates performance after removing high/low valued samples\n",
    "remove_high_low_performance = remove_high_low(dve_out, eval_model, x_train, y_train, \n",
    "                                              x_valid, y_valid, x_test, y_test, 'accuracy', plot = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Corrupted sample discovery\n",
    "\n",
    "For our synthetically-generated noisy training dataset, we can assess the performance of our method in **finding the noisy samples by using the known noise indices**. Note that unlike the first two evaluations, this cell is only for academic purposes because you need the ground truth noisy sample indices so if users come with their own .csv files, they cannot use this cell. \n",
    "\n",
    " * Report True Positive Rates (TPR) of corrupted sample discovery.\n",
    " * Visualize the results using line graphs (set plot = True).\n",
    " * x-axis: Portions of inspected samples.\n",
    " * y-axis: True positive rates (TPR) of corrupted sample discovery.\n",
    " * Blue line: DVRL, Orange line: Optimal, Green line: Random."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If noise_rate is positive value.\n",
    "if noise_rate > 0:\n",
    "    \n",
    "    # Evaluates true positive rates (TPR) of corrupted sample discovery and plot TPR\n",
    "    noise_discovery_performance = discover_corrupted_sample(dve_out, noise_idx, noise_rate, plot = True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
